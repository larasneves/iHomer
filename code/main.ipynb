{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/llns2/Library/CloudStorage/OneDrive-UniversidadedoPorto/Mestrado/MECD/Tese/mlhat_Lara/src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.chdir(\"/lustre/home/pintosanel/mlhat/src\")  \n",
    "os.chdir(\"/Users/llns2/Library/CloudStorage/OneDrive-UniversidadedoPorto/Mestrado/MECD/Tese/mlhat_Lara/src\")  \n",
    "\n",
    "print(os.getcwd())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasetsmultioutput import *\n",
    "from multioutput import MLHAT\n",
    "import random\n",
    "from river import metrics\n",
    "from river.metrics import F1\n",
    "from river.metrics.base import Metrics\n",
    "from river.metrics.multioutput import ExactMatch, MacroAverage, MicroAverage\n",
    "import evaluate\n",
    "\n",
    "import pandas as pd\n",
    "from river import ensemble\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odac2 import ODAC2  # Import the class from the .py file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import river.base\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MajorityLabelset(river.base.Estimator):\n",
    "    \"\"\"\n",
    "    Majority Labelset classifier for multi-label classification.\n",
    "    Each observed labelset (e.g., [0,0,1,1,0,0]) is treated as a single class value.\n",
    "    The classifier always predicts the most frequently observed labelset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vector_counts = {}  # Dictionary to store labelset frequencies\n",
    "        self.majority_labelset = None\n",
    "        self.max_value = -1.0\n",
    "\n",
    "    def learn_one(self, x, y):\n",
    "        \"\"\"Trains the classifier on a single instance.\"\"\"\n",
    "        y_tuple = tuple(y.values())  # Convert dictionary to tuple for hashing\n",
    "\n",
    "        # Count occurrences of each labelset\n",
    "        freq = self.vector_counts.get(y_tuple, 0) + 1\n",
    "        self.vector_counts[y_tuple] = freq\n",
    "\n",
    "        # Update majority labelset if necessary\n",
    "        if freq > self.max_value:\n",
    "            self.max_value = freq\n",
    "            self.majority_labelset = y_tuple\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        \"\"\"Returns the most frequently observed labelset.\"\"\"\n",
    "        if self.majority_labelset is None:\n",
    "            return {label: 0.0 for label in x}  # Default to all zeros if no data has been seen\n",
    "\n",
    "        return {label: float(val) for label, val in zip(x.keys(), self.majority_labelset)}\n",
    "\n",
    "    def get_purpose_string(self):\n",
    "        \"\"\"Returns a description of the classifier.\"\"\"\n",
    "        return \"Majority labelset classifier: always predicts the label vector most frequently seen so far.\"\n",
    "\n",
    "    def reset_learning(self):\n",
    "        \"\"\"Resets the classifier to its initial state.\"\"\"\n",
    "        self.vector_counts.clear()\n",
    "        self.majority_labelset = None\n",
    "        self.max_value = -1.0\n",
    "\n",
    "    def get_model_measurements(self):\n",
    "        \"\"\"Returns model measurements (currently not implemented).\"\"\"\n",
    "        return None\n",
    "\n",
    "    def is_randomizable(self):\n",
    "        \"\"\"Indicates whether the classifier has a random component (it does not).\"\"\"\n",
    "        return False\n",
    "\n",
    "    def get_model_description(self):\n",
    "        \"\"\"Returns a textual description of the model.\"\"\"\n",
    "        return str(self.majority_labelset) if self.majority_labelset else \"No data seen yet.\"\n",
    "\n",
    "    def get_frequency_table(self):\n",
    "        \"\"\"Returns the frequency table of labelsets.\"\"\"\n",
    "        return self.vector_counts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "def compute_co_occurrence(freq_table):\n",
    "    \"\"\"\n",
    "    Computes co-occurrence frequency of individual labels and label pairs.\n",
    "    \n",
    "    Parameters:\n",
    "    - freq_table (dict): Majority labelset frequency table.\n",
    "    \n",
    "    Returns:\n",
    "    - co_occurrence_count (dict): Dictionary counting co-occurrences of label pairs.\n",
    "    \"\"\"\n",
    "    co_occurrence_count = defaultdict(int)\n",
    "    \n",
    "    for labelset, freq in freq_table.items():\n",
    "        # Convert labelset to index positions where labels are True\n",
    "        active_labels = [j for j, val in enumerate(labelset) if val]\n",
    "\n",
    "        # Count individual label occurrences\n",
    "        for label in active_labels:\n",
    "            co_occurrence_count[(label,)] += freq\n",
    "        \n",
    "        # Count pairwise and higher-order co-occurrences\n",
    "        for pair in itertools.combinations(active_labels, 2):  # Change 2 to 3 for triplets\n",
    "            co_occurrence_count[pair] += freq\n",
    "    \n",
    "    return co_occurrence_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_labelset_sizes(freq_table):\n",
    "    \"\"\"Extracts the distribution of labelset sizes.\"\"\"\n",
    "    size_distribution = Counter()\n",
    "    for labelset, freq in freq_table.items():\n",
    "        label_count = sum(labelset)  \n",
    "        size_distribution[label_count] += freq\n",
    "    return size_distribution\n",
    "\n",
    "def get_safe_min_cluster_size(size_distribution):\n",
    "    \"\"\"Determines a safe minimum cluster size based on frequency distribution.\"\"\"\n",
    "    if not size_distribution:\n",
    "        return 1\n",
    "    return max(size_distribution, key=size_distribution.get) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_split_clusters(odac2, cluster_dict, min_size, max_size, output_dir, i, color_list):\n",
    "   \n",
    "    merged_by_level ={}\n",
    "    \n",
    "    assigned_labels = set()\n",
    "    color_idx = 0\n",
    "    cluster_colors = {}\n",
    "    \n",
    "    clusters_with_level = [(path, info['members'], info['level'], info['parent'], info['siblings']) \n",
    "                          for path, info in cluster_dict.items()]\n",
    "    \n",
    "    \n",
    "    max_iterations = 10  # Safety to prevent infinite loops\n",
    "    iteration = 0\n",
    "    \n",
    "    # Force aggressive merging if needed\n",
    "    force_merge_all_small = False\n",
    "    \n",
    "    while iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        print(f\"Starting iteration {iteration}\")\n",
    "        \n",
    "        # Identify small clusters in this iteration\n",
    "        small_clusters = [(path, members, level, parent, siblings) \n",
    "                         for path, members, level, parent, siblings in clusters_with_level \n",
    "                         if len(members) < min_size and not any(m in assigned_labels for m in members)]\n",
    "        \n",
    "        if not small_clusters:\n",
    "            # print(\"No more small clusters found. Merging complete.\")\n",
    "            break\n",
    "            \n",
    "        print(f\"Found {len(small_clusters)} small clusters in iteration {iteration}\")\n",
    "        \n",
    "        # Group small clusters by level\n",
    "        clusters_by_level = {}\n",
    "        for path, members, level, parent, siblings in small_clusters:\n",
    "            if level not in clusters_by_level:\n",
    "                clusters_by_level[level] = []\n",
    "            clusters_by_level[level].append((path, members, parent, siblings))\n",
    "        \n",
    "        # Sort levels to process deepest levels first (higher level number)\n",
    "        levels = sorted(clusters_by_level.keys(), reverse=True)\n",
    "        \n",
    "        merged_any = False  \n",
    "        merged_by_level = {}  \n",
    "        \n",
    "        if iteration > max_iterations/2 and small_clusters:\n",
    "            # If we're in later iterations and still have small clusters, \n",
    "            # be more aggressive with merging\n",
    "            force_merge_all_small = True\n",
    "        \n",
    "        # Process each level\n",
    "        for level in levels:\n",
    "            level_clusters = clusters_by_level[level]\n",
    "            \n",
    "            # First, identify true sibling pairs using path information\n",
    "            sibling_pairs = []\n",
    "            processed_paths = set()\n",
    "            \n",
    "            for path1, members1, parent1, siblings1 in level_clusters:\n",
    "                if path1 in processed_paths:\n",
    "                    continue\n",
    "                    \n",
    "                # Find siblings of this cluster\n",
    "                for sibling_path in siblings1:\n",
    "                    sibling_entry = next((entry for entry in level_clusters if entry[0] == sibling_path \n",
    "                                         and sibling_path not in processed_paths), None)\n",
    "                    if sibling_entry:\n",
    "                        # Found a small sibling - create a pair\n",
    "                        sibling_pair = ((path1, members1), (sibling_entry[0], sibling_entry[1]))\n",
    "                        sibling_pairs.append(sibling_pair)\n",
    "                        processed_paths.add(path1)\n",
    "                        processed_paths.add(sibling_path)\n",
    "                        break\n",
    "            \n",
    "            # Process sibling pairs at this level\n",
    "            for (path1, members1), (path2, members2) in sibling_pairs:\n",
    "                merged_members = members1.union(members2)\n",
    "                \n",
    "                # print(f\"Merging true sibling clusters: {path1} and {path2}\")\n",
    "              \n",
    "                assigned_color = color_list[color_idx]\n",
    "                color_idx = (color_idx + 1) % len(color_list)\n",
    "                \n",
    "                cluster_colors[tuple(sorted(members1))] = assigned_color\n",
    "                cluster_colors[tuple(sorted(members2))] = assigned_color\n",
    "                cluster_colors[tuple(sorted(merged_members))] = assigned_color\n",
    "                \n",
    "                if level not in merged_by_level:\n",
    "                    merged_by_level[level] = []\n",
    "                merged_by_level[level].append(list(merged_members))\n",
    "                \n",
    "                # Mark these labels as assigned\n",
    "                assigned_labels.update(merged_members)\n",
    "                merged_any = True\n",
    "            \n",
    "            # Handle remaining small clusters at this level that had no siblings\n",
    "            remaining_small_clusters = [\n",
    "                (path, members, parent) \n",
    "                for path, members, parent, _ in level_clusters \n",
    "                if path not in processed_paths\n",
    "            ]\n",
    "            \n",
    "            # Try to merge remaining small clusters with their parent's other children\n",
    "            for path, members, parent_path in remaining_small_clusters:\n",
    "                if any(m in assigned_labels for m in members):\n",
    "                    continue  # Skip if already processed\n",
    "                    \n",
    "                merged = False\n",
    "                if parent_path:\n",
    "                    # Find other children of the same parent that aren't already assigned\n",
    "                    potential_siblings = [\n",
    "                        (p, mem)\n",
    "                        for p, mem, lvl, par, _ in clusters_with_level\n",
    "                        if par == parent_path and p != path and not any(m in assigned_labels for m in mem)\n",
    "                    ]\n",
    "                    \n",
    "                    for sibling_path, sibling_members in potential_siblings:\n",
    "                        if len(members) + len(sibling_members) <= max_size:\n",
    "                            merged_members = members.union(sibling_members)\n",
    "                            \n",
    "                            # print(f\"Merging small cluster with parent sibling: {path} and {sibling_path}\")\n",
    "                            \n",
    "                            assigned_color = color_list[color_idx]\n",
    "                            color_idx = (color_idx + 1) % len(color_list)\n",
    "                            \n",
    "                            cluster_colors[tuple(sorted(members))] = assigned_color\n",
    "                            cluster_colors[tuple(sorted(sibling_members))] = assigned_color\n",
    "                            cluster_colors[tuple(sorted(merged_members))] = assigned_color\n",
    "                            \n",
    "                            if level not in merged_by_level:\n",
    "                                merged_by_level[level] = []\n",
    "                            merged_by_level[level].append(list(merged_members))\n",
    "                            \n",
    "                            assigned_labels.update(merged_members)\n",
    "                            merged = True\n",
    "                            merged_any = True\n",
    "                            break\n",
    "                \n",
    "                if not merged:\n",
    "                    # Sort levels from lowest to current to try merging with most hierarchically similar first\n",
    "                    merger_levels = sorted([l for l in merged_by_level.keys()], key=lambda x: abs(x - level))\n",
    "                    \n",
    "                    for target_level in merger_levels:\n",
    "                        for idx, merged_cluster_list in enumerate(merged_by_level[target_level]):\n",
    "                            merged_cluster = set(merged_cluster_list)\n",
    "                            if len(merged_cluster) + len(members) <= max_size:\n",
    "                                # print(f\"Merging small cluster with existing merged cluster: {path}\")\n",
    "                                \n",
    "                                # Preserve color of the existing merged cluster\n",
    "                                existing_cluster_key = tuple(sorted(merged_cluster_list))\n",
    "                                if existing_cluster_key in cluster_colors:\n",
    "                                    assigned_color = cluster_colors[existing_cluster_key]\n",
    "                                else:\n",
    "                                    assigned_color = color_list[color_idx]\n",
    "                                    color_idx = (color_idx + 1) % len(color_list)\n",
    "                                \n",
    "                                # Update cluster\n",
    "                                merged_cluster.update(members)\n",
    "                                merged_by_level[target_level][idx] = list(merged_cluster)\n",
    "                                \n",
    "                                # print(f\"Updated merged cluster: {merged_cluster}\")\n",
    "                                \n",
    "                                # Assign colors\n",
    "                                cluster_colors[tuple(sorted(members))] = assigned_color\n",
    "                                cluster_colors[tuple(sorted(merged_cluster))] = assigned_color\n",
    "                                \n",
    "                                assigned_labels.update(members)\n",
    "                                merged = True\n",
    "                                merged_any = True\n",
    "                                break\n",
    "                        \n",
    "                        if merged:\n",
    "                            break\n",
    "                \n",
    "                if not merged:\n",
    "                    if not force_merge_all_small:\n",
    "                        if level not in merged_by_level:\n",
    "                            merged_by_level[level] = []\n",
    "                        merged_by_level[level].append(list(members))\n",
    "                        \n",
    "                        assigned_color = color_list[color_idx]\n",
    "                        color_idx = (color_idx + 1) % len(color_list)\n",
    "                        cluster_colors[tuple(sorted(members))] = assigned_color\n",
    "                    else:\n",
    "                        all_merger_candidates = []\n",
    "                        \n",
    "                        for l, clusters in merged_by_level.items():\n",
    "                            for idx, cluster_list in enumerate(clusters):\n",
    "                                cluster = set(cluster_list)\n",
    "                                if len(cluster) + len(members) <= max_size:\n",
    "                                    distance = abs(l - level)\n",
    "                                    all_merger_candidates.append((distance, l, idx, cluster))\n",
    "                        \n",
    "                        all_merger_candidates.sort()\n",
    "                        \n",
    "                        if all_merger_candidates:\n",
    "                            distance, target_level, idx, target_cluster = all_merger_candidates[0]\n",
    "                            \n",
    "                            print(f\"FORCED MERGE: Small cluster {path} with level {target_level} cluster\")\n",
    "                            \n",
    "                            existing_cluster_key = tuple(sorted(target_cluster))\n",
    "                            if existing_cluster_key in cluster_colors:\n",
    "                                assigned_color = cluster_colors[existing_cluster_key]\n",
    "                            else:\n",
    "                                assigned_color = color_list[color_idx]\n",
    "                                color_idx = (color_idx + 1) % len(color_list)\n",
    "                            \n",
    "                            target_cluster.update(members)\n",
    "                            merged_by_level[target_level][idx] = list(target_cluster)\n",
    "                            \n",
    "                            cluster_colors[tuple(sorted(members))] = assigned_color\n",
    "                            cluster_colors[tuple(sorted(target_cluster))] = assigned_color\n",
    "                            \n",
    "                            assigned_labels.update(members)\n",
    "                            merged = True\n",
    "                            merged_any = True\n",
    "                        else:\n",
    "                            if level not in merged_by_level:\n",
    "                                merged_by_level[level] = []\n",
    "                            merged_by_level[level].append(list(members))\n",
    "                            \n",
    "                            assigned_color = color_list[color_idx]\n",
    "                            color_idx = (color_idx + 1) % len(color_list)\n",
    "                            cluster_colors[tuple(sorted(members))] = assigned_color\n",
    "        \n",
    "        if not merged_any:\n",
    "            print(\"No merges possible. Some clusters may still be below minimum size.\")\n",
    "            break\n",
    "    \n",
    "        clusters_with_level = []\n",
    "        \n",
    "        for level, clusters in merged_by_level.items():\n",
    "            for cluster_members in clusters:\n",
    "                dummy_path = f\"merged_level_{level}_{len(clusters_with_level)}\"\n",
    "                clusters_with_level.append((dummy_path, set(cluster_members), level, None, []))\n",
    "    \n",
    "    final_merged_by_level = {}\n",
    "    for level, clusters in merged_by_level.items() :\n",
    "        final_merged_by_level[level] = clusters\n",
    "    \n",
    "    small_clusters_remaining = []\n",
    "    for level, clusters in final_merged_by_level.items():\n",
    "        for cluster in clusters:\n",
    "            if len(cluster) < min_size:\n",
    "                small_clusters_remaining.append((level, cluster))\n",
    "            \n",
    "            if small_clusters_remaining:\n",
    "                print(f\"WARNING: {len(small_clusters_remaining)} clusters still below minimum size after merging:\")\n",
    "                for level, cluster in small_clusters_remaining:\n",
    "                    print(f\"  Level {level}, size {len(cluster)}: {cluster}\")\n",
    "                \n",
    "                if force_merge_all_small:\n",
    "                    remaining_small = []\n",
    "                    for level, cluster in small_clusters_remaining:\n",
    "                        remaining_small.append((level, set(cluster)))\n",
    "                    \n",
    "\n",
    "                if small_clusters_remaining:\n",
    "                    print(f\"Force merging {len(small_clusters_remaining)} remaining small clusters...\")\n",
    "\n",
    "                    for level, small_cluster in small_clusters_remaining:\n",
    "                        cluster_key = None\n",
    "                        \n",
    "                        for key, value in cluster_dict.items(): \n",
    "                            if value[\"members\"] == set(small_cluster):\n",
    "                                cluster_key = key\n",
    "                                break\n",
    "\n",
    "                        if cluster_key and \"siblings\" in cluster_dict[cluster_key]:  \n",
    "                            siblings = cluster_dict[cluster_key][\"siblings\"]\n",
    "                            \n",
    "                            for sibling_key in siblings:\n",
    "                                if sibling_key in cluster_dict:\n",
    "                                    sibling_cluster = cluster_dict[sibling_key][\"members\"]\n",
    "                                    \n",
    "                                    if len(sibling_cluster) >= min_size:\n",
    "                                        print(f\"Merging small cluster {small_cluster} with its sibling {sibling_cluster}\")\n",
    "                                        \n",
    "                                        small_cluster_tuple = tuple(sorted(small_cluster))\n",
    "                                        sibling_cluster_tuple = tuple(sorted(sibling_cluster))\n",
    "\n",
    "                                        if small_cluster_tuple not in cluster_colors:\n",
    "                                            cluster_colors[small_cluster_tuple] = color_list[color_idx]  \n",
    "\n",
    "                                        if sibling_cluster_tuple not in cluster_colors:\n",
    "                                            cluster_colors[sibling_cluster_tuple] = cluster_colors[small_cluster_tuple] \n",
    "\n",
    "                                        cluster_dict[sibling_key][\"members\"].update(small_cluster)\n",
    "                                        \n",
    "                                        del cluster_dict[cluster_key]  \n",
    "\n",
    "                                        break\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        print(\"All clusters meet the minimum size requirement.\")\n",
    "    \n",
    "    for level in list(final_merged_by_level.keys()):\n",
    "        final_merged_by_level[level] = [cluster for cluster in final_merged_by_level[level] if cluster]\n",
    "    \n",
    "    clusters = sum(final_merged_by_level.values(), [])\n",
    "\n",
    "    tree_image_path = os.path.join(output_dir, f\"tree_structure_{i}_new\")\n",
    "    dot = odac2.draw(show_clusters_info=[\"timeseries_names\"], cluster_colors=cluster_colors)\n",
    "    dot.render(tree_image_path, format=\"png\", cleanup=True)\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_dict(self):\n",
    "    cluster_dict = {}\n",
    "\n",
    "    for cluster in self._find_all_active_clusters(self._root_node):\n",
    "        level = '_'.join(cluster.name.split('_')[1:3])  # Get \"LVL_1\", \"LVL_2\", etc.\n",
    "        \n",
    "        if level not in cluster_dict:\n",
    "            cluster_dict[level] = []\n",
    "        cluster_dict[level].append(set(cluster.timeseries_names)) \n",
    "\n",
    "    return cluster_dict\n",
    "\n",
    "def sort_cluster_levels(cluster_dict): #Sorts the cluster levels numerically, based on the level number after \"LVL_\".\n",
    "    \n",
    "    def extract_level_number(cluster_name):\n",
    "        if \"_LVL_\" in cluster_name:\n",
    "            parts = cluster_name.split(\"_LVL_\")\n",
    "            level_part = parts[1].split(\"_\")[0]  \n",
    "            return int(level_part)  \n",
    "        elif cluster_name == \"ROOT\":\n",
    "            return float('inf')  \n",
    "        return float('inf') \n",
    "\n",
    "    # Sort the levels by their numeric part, with special handling for ROOT and unknown formats\n",
    "    sorted_levels = sorted(cluster_dict.keys(), key=extract_level_number, reverse=True)\n",
    "    return sorted_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT cluster initialized\n",
      "Cluster LVL_1_ROOT_0: ['Class1', 'Class10', 'Class12', 'Class2', 'Class3', 'Class4', 'Class5', 'Class8', 'Class9']\n",
      "Cluster LVL_1_ROOT_1: ['Class11', 'Class13', 'Class14', 'Class6', 'Class7']\n",
      "----------------\n",
      "Cluster LVL_2_ROOT_0_0: ['Class1', 'Class10', 'Class12', 'Class3', 'Class8', 'Class9']\n",
      "Cluster LVL_2_ROOT_0_1: ['Class2', 'Class4', 'Class5']\n",
      "Cluster LVL_2_ROOT_1_0: ['Class13', 'Class6']\n",
      "Cluster LVL_2_ROOT_1_1: ['Class11', 'Class14', 'Class7']\n",
      "Structure changed at observation 200\n",
      "Labelset Size Distribution: {4: 91, 2: 36, 3: 7, 5: 12, 7: 14, 6: 35, 8: 4, 1: 1}\n",
      "Recommended Minimum Cluster Size: 4\n",
      "Starting iteration 1\n",
      "Found 3 small clusters in iteration 1\n",
      "Merging true sibling clusters: LVL_2_ROOT_1_0 and LVL_2_ROOT_1_1\n",
      "Merging small cluster with parent sibling: LVL_2_ROOT_0_1 and LVL_2_ROOT_0_0\n",
      "Starting iteration 2\n",
      "All clusters meet the minimum size requirement.\n",
      "Refined Clusters: [['Class6', 'Class7', 'Class13', 'Class14', 'Class11'], ['Class4', 'Class1', 'Class9', 'Class8', 'Class12', 'Class2', 'Class3', 'Class10', 'Class5']]\n",
      "----------------\n",
      "Cluster LVL_3_ROOT_0_0_0: ['Class10', 'Class12', 'Class3', 'Class8']\n",
      "Cluster LVL_3_ROOT_0_0_1: ['Class1', 'Class9']\n",
      "Cluster LVL_3_ROOT_0_1_0: ['Class2', 'Class4']\n",
      "Cluster LVL_3_ROOT_0_1_1: ['Class5']\n",
      "Cluster LVL_2_ROOT_1_0: ['Class13', 'Class6']\n",
      "Cluster LVL_2_ROOT_1_1: ['Class11', 'Class14', 'Class7']\n",
      "Structure changed at observation 300\n",
      "Labelset Size Distribution: {4: 143, 2: 52, 3: 11, 5: 15, 7: 21, 6: 49, 8: 8, 1: 1}\n",
      "Recommended Minimum Cluster Size: 4\n",
      "Starting iteration 1\n",
      "Found 5 small clusters in iteration 1\n",
      "Merging true sibling clusters: LVL_3_ROOT_0_1_0 and LVL_3_ROOT_0_1_1\n",
      "Merging small cluster with parent sibling: LVL_3_ROOT_0_0_1 and LVL_3_ROOT_0_0_0\n",
      "Merging true sibling clusters: LVL_2_ROOT_1_0 and LVL_2_ROOT_1_1\n",
      "Starting iteration 2\n",
      "WARNING: 1 clusters still below minimum size after merging:\n",
      "  Level 3, size 3: ['Class2', 'Class4', 'Class5']\n",
      "Force merging 1 remaining small clusters...\n",
      "WARNING: 1 clusters still below minimum size after merging:\n",
      "  Level 3, size 3: ['Class2', 'Class4', 'Class5']\n",
      "Force merging 1 remaining small clusters...\n",
      "WARNING: 1 clusters still below minimum size after merging:\n",
      "  Level 3, size 3: ['Class2', 'Class4', 'Class5']\n",
      "Force merging 1 remaining small clusters...\n",
      "All clusters meet the minimum size requirement.\n",
      "Refined Clusters: [['Class2', 'Class4', 'Class5'], ['Class1', 'Class9', 'Class3', 'Class8', 'Class10', 'Class12'], ['Class6', 'Class7', 'Class13', 'Class14', 'Class11']]\n",
      "----------------\n",
      "Cluster LVL_3_ROOT_0_0_0: ['Class10', 'Class12', 'Class3', 'Class8']\n",
      "Cluster LVL_3_ROOT_0_0_1: ['Class1', 'Class9']\n",
      "Cluster LVL_2_ROOT_0_1: ['Class2', 'Class4', 'Class5']\n",
      "Cluster LVL_2_ROOT_1_0: ['Class13', 'Class6']\n",
      "Cluster LVL_2_ROOT_1_1: ['Class11', 'Class14', 'Class7']\n",
      "Structure changed at observation 400\n",
      "Labelset Size Distribution: {4: 181, 2: 72, 3: 18, 5: 22, 7: 26, 6: 69, 8: 9, 1: 1, 9: 2}\n",
      "Recommended Minimum Cluster Size: 4\n",
      "Starting iteration 1\n",
      "Found 4 small clusters in iteration 1\n",
      "Merging small cluster with parent sibling: LVL_3_ROOT_0_0_1 and LVL_3_ROOT_0_0_0\n",
      "Merging true sibling clusters: LVL_2_ROOT_1_0 and LVL_2_ROOT_1_1\n",
      "Merging small cluster with existing merged cluster: LVL_2_ROOT_0_1\n",
      "Starting iteration 2\n",
      "All clusters meet the minimum size requirement.\n",
      "Refined Clusters: [['Class1', 'Class9', 'Class3', 'Class8', 'Class10', 'Class12'], ['Class6', 'Class4', 'Class7', 'Class14', 'Class11', 'Class2', 'Class13', 'Class5']]\n",
      "----------------\n",
      "Cluster LVL_4_ROOT_0_0_0_0: ['Class10', 'Class12']\n",
      "Cluster LVL_4_ROOT_0_0_0_1: ['Class3', 'Class8']\n",
      "Cluster LVL_3_ROOT_0_0_1: ['Class1', 'Class9']\n",
      "Cluster LVL_3_ROOT_0_1_0: ['Class2', 'Class4']\n",
      "Cluster LVL_3_ROOT_0_1_1: ['Class5']\n",
      "Cluster LVL_2_ROOT_1_0: ['Class13', 'Class6']\n",
      "Cluster LVL_2_ROOT_1_1: ['Class11', 'Class14', 'Class7']\n",
      "Structure changed at observation 500\n",
      "Labelset Size Distribution: {4: 233, 2: 86, 3: 22, 5: 30, 7: 28, 6: 89, 8: 9, 1: 1, 9: 2}\n",
      "Recommended Minimum Cluster Size: 4\n",
      "Starting iteration 1\n",
      "Found 7 small clusters in iteration 1\n",
      "Merging true sibling clusters: LVL_4_ROOT_0_0_0_0 and LVL_4_ROOT_0_0_0_1\n",
      "Merging true sibling clusters: LVL_3_ROOT_0_1_0 and LVL_3_ROOT_0_1_1\n",
      "Merging small cluster with existing merged cluster: LVL_3_ROOT_0_0_1\n",
      "Merging true sibling clusters: LVL_2_ROOT_1_0 and LVL_2_ROOT_1_1\n",
      "Starting iteration 2\n",
      "All clusters meet the minimum size requirement.\n",
      "Refined Clusters: [['Class10', 'Class3', 'Class12', 'Class8'], ['Class2', 'Class4', 'Class1', 'Class9', 'Class5'], ['Class6', 'Class7', 'Class13', 'Class14', 'Class11']]\n",
      "----------------\n",
      "Cluster LVL_4_ROOT_0_0_0_0: ['Class10', 'Class12']\n",
      "Cluster LVL_4_ROOT_0_0_0_1: ['Class3', 'Class8']\n",
      "Cluster LVL_3_ROOT_0_0_1: ['Class1', 'Class9']\n",
      "Cluster LVL_2_ROOT_0_1: ['Class2', 'Class4', 'Class5']\n",
      "Cluster LVL_2_ROOT_1_0: ['Class13', 'Class6']\n",
      "Cluster LVL_2_ROOT_1_1: ['Class11', 'Class14', 'Class7']\n",
      "Structure changed at observation 600\n",
      "Labelset Size Distribution: {4: 274, 2: 100, 3: 27, 5: 41, 7: 31, 6: 107, 8: 15, 1: 3, 9: 2}\n",
      "Recommended Minimum Cluster Size: 4\n",
      "Starting iteration 1\n",
      "Found 6 small clusters in iteration 1\n",
      "Merging true sibling clusters: LVL_4_ROOT_0_0_0_0 and LVL_4_ROOT_0_0_0_1\n",
      "Merging small cluster with existing merged cluster: LVL_3_ROOT_0_0_1\n",
      "Merging true sibling clusters: LVL_2_ROOT_1_0 and LVL_2_ROOT_1_1\n",
      "Merging small cluster with existing merged cluster: LVL_2_ROOT_0_1\n",
      "Starting iteration 2\n",
      "All clusters meet the minimum size requirement.\n",
      "Refined Clusters: [['Class3', 'Class1', 'Class9', 'Class8', 'Class10', 'Class12'], ['Class6', 'Class4', 'Class7', 'Class14', 'Class11', 'Class2', 'Class13', 'Class5']]\n",
      "----------------\n",
      "Cluster LVL_4_ROOT_0_0_0_0: ['Class10', 'Class12']\n",
      "Cluster LVL_4_ROOT_0_0_0_1: ['Class3', 'Class8']\n",
      "Cluster LVL_3_ROOT_0_0_1: ['Class1', 'Class9']\n",
      "Cluster LVL_3_ROOT_0_1_0: ['Class2', 'Class4']\n",
      "Cluster LVL_3_ROOT_0_1_1: ['Class5']\n",
      "Cluster LVL_2_ROOT_1_0: ['Class13', 'Class6']\n",
      "Cluster LVL_2_ROOT_1_1: ['Class11', 'Class14', 'Class7']\n",
      "Structure changed at observation 700\n",
      "Labelset Size Distribution: {4: 317, 2: 117, 3: 31, 5: 47, 7: 35, 6: 130, 8: 15, 1: 5, 9: 3}\n",
      "Recommended Minimum Cluster Size: 4\n",
      "Starting iteration 1\n",
      "Found 7 small clusters in iteration 1\n",
      "Merging true sibling clusters: LVL_4_ROOT_0_0_0_0 and LVL_4_ROOT_0_0_0_1\n",
      "Merging true sibling clusters: LVL_3_ROOT_0_1_0 and LVL_3_ROOT_0_1_1\n",
      "Merging small cluster with existing merged cluster: LVL_3_ROOT_0_0_1\n",
      "Merging true sibling clusters: LVL_2_ROOT_1_0 and LVL_2_ROOT_1_1\n",
      "Starting iteration 2\n",
      "All clusters meet the minimum size requirement.\n",
      "Refined Clusters: [['Class10', 'Class3', 'Class12', 'Class8'], ['Class2', 'Class4', 'Class1', 'Class9', 'Class5'], ['Class6', 'Class7', 'Class13', 'Class14', 'Class11']]\n",
      "----------------\n",
      "Cluster LVL_4_ROOT_0_0_0_0: ['Class10', 'Class12']\n",
      "Cluster LVL_4_ROOT_0_0_0_1: ['Class3', 'Class8']\n",
      "Cluster LVL_3_ROOT_0_0_1: ['Class1', 'Class9']\n",
      "Cluster LVL_2_ROOT_0_1: ['Class2', 'Class4', 'Class5']\n",
      "Cluster LVL_2_ROOT_1_0: ['Class13', 'Class6']\n",
      "Cluster LVL_2_ROOT_1_1: ['Class11', 'Class14', 'Class7']\n",
      "Structure changed at observation 800\n",
      "Labelset Size Distribution: {4: 362, 2: 136, 3: 38, 5: 54, 7: 35, 6: 150, 8: 16, 1: 6, 9: 3}\n",
      "Recommended Minimum Cluster Size: 4\n",
      "Starting iteration 1\n",
      "Found 6 small clusters in iteration 1\n",
      "Merging true sibling clusters: LVL_4_ROOT_0_0_0_0 and LVL_4_ROOT_0_0_0_1\n",
      "Merging small cluster with existing merged cluster: LVL_3_ROOT_0_0_1\n",
      "Merging true sibling clusters: LVL_2_ROOT_1_0 and LVL_2_ROOT_1_1\n",
      "Merging small cluster with existing merged cluster: LVL_2_ROOT_0_1\n",
      "Starting iteration 2\n",
      "All clusters meet the minimum size requirement.\n",
      "Refined Clusters: [['Class3', 'Class1', 'Class9', 'Class8', 'Class10', 'Class12'], ['Class6', 'Class4', 'Class7', 'Class14', 'Class11', 'Class2', 'Class13', 'Class5']]\n",
      "----------------\n",
      "Cluster LVL_4_ROOT_0_0_0_0: ['Class10', 'Class12']\n",
      "Cluster LVL_4_ROOT_0_0_0_1: ['Class3', 'Class8']\n",
      "Cluster LVL_3_ROOT_0_0_1: ['Class1', 'Class9']\n",
      "Cluster LVL_3_ROOT_0_1_0: ['Class2', 'Class4']\n",
      "Cluster LVL_3_ROOT_0_1_1: ['Class5']\n",
      "Cluster LVL_2_ROOT_1_0: ['Class13', 'Class6']\n",
      "Cluster LVL_2_ROOT_1_1: ['Class11', 'Class14', 'Class7']\n",
      "Structure changed at observation 900\n",
      "Labelset Size Distribution: {4: 413, 2: 156, 3: 42, 5: 58, 7: 37, 6: 163, 8: 18, 1: 7, 9: 4, 10: 2}\n",
      "Recommended Minimum Cluster Size: 4\n",
      "Starting iteration 1\n",
      "Found 7 small clusters in iteration 1\n",
      "Merging true sibling clusters: LVL_4_ROOT_0_0_0_0 and LVL_4_ROOT_0_0_0_1\n",
      "Merging true sibling clusters: LVL_3_ROOT_0_1_0 and LVL_3_ROOT_0_1_1\n",
      "Merging small cluster with existing merged cluster: LVL_3_ROOT_0_0_1\n",
      "Merging true sibling clusters: LVL_2_ROOT_1_0 and LVL_2_ROOT_1_1\n",
      "Starting iteration 2\n",
      "All clusters meet the minimum size requirement.\n",
      "Refined Clusters: [['Class10', 'Class3', 'Class12', 'Class8'], ['Class2', 'Class4', 'Class1', 'Class9', 'Class5'], ['Class6', 'Class7', 'Class13', 'Class14', 'Class11']]\n",
      "----------------\n",
      "Cluster LVL_4_ROOT_0_0_0_0: ['Class10', 'Class12']\n",
      "Cluster LVL_4_ROOT_0_0_0_1: ['Class3', 'Class8']\n",
      "Cluster LVL_3_ROOT_0_0_1: ['Class1', 'Class9']\n",
      "Cluster LVL_2_ROOT_0_1: ['Class2', 'Class4', 'Class5']\n",
      "Cluster LVL_2_ROOT_1_0: ['Class13', 'Class6']\n",
      "Cluster LVL_3_ROOT_1_1_0: ['Class11', 'Class14']\n",
      "Cluster LVL_3_ROOT_1_1_1: ['Class7']\n",
      "Structure changed at observation 1000\n",
      "Labelset Size Distribution: {4: 453, 2: 176, 3: 48, 5: 66, 7: 45, 6: 178, 8: 20, 1: 8, 9: 4, 10: 2}\n",
      "Recommended Minimum Cluster Size: 4\n",
      "Starting iteration 1\n",
      "Found 7 small clusters in iteration 1\n",
      "Merging true sibling clusters: LVL_4_ROOT_0_0_0_0 and LVL_4_ROOT_0_0_0_1\n",
      "Merging true sibling clusters: LVL_3_ROOT_1_1_0 and LVL_3_ROOT_1_1_1\n",
      "Merging small cluster with existing merged cluster: LVL_3_ROOT_0_0_1\n",
      "Merging small cluster with existing merged cluster: LVL_2_ROOT_0_1\n",
      "Merging small cluster with existing merged cluster: LVL_2_ROOT_1_0\n",
      "Starting iteration 2\n",
      "All clusters meet the minimum size requirement.\n",
      "Refined Clusters: [['Class10', 'Class3', 'Class12', 'Class8'], ['Class6', 'Class4', 'Class1', 'Class9', 'Class7', 'Class14', 'Class11', 'Class2', 'Class13', 'Class5']]\n",
      "----------------\n",
      "Cluster LVL_4_ROOT_0_0_0_0: ['Class10', 'Class12']\n",
      "Cluster LVL_4_ROOT_0_0_0_1: ['Class3', 'Class8']\n",
      "Cluster LVL_3_ROOT_0_0_1: ['Class1', 'Class9']\n",
      "Cluster LVL_3_ROOT_0_1_0: ['Class2', 'Class4']\n",
      "Cluster LVL_3_ROOT_0_1_1: ['Class5']\n",
      "Cluster LVL_2_ROOT_1_0: ['Class13', 'Class6']\n",
      "Cluster LVL_3_ROOT_1_1_0: ['Class11', 'Class14']\n",
      "Cluster LVL_3_ROOT_1_1_1: ['Class7']\n",
      "Structure changed at observation 1100\n",
      "Labelset Size Distribution: {4: 506, 2: 194, 3: 53, 5: 71, 7: 50, 6: 189, 8: 21, 1: 10, 9: 4, 10: 2}\n",
      "Recommended Minimum Cluster Size: 4\n",
      "Starting iteration 1\n",
      "Found 8 small clusters in iteration 1\n",
      "Merging true sibling clusters: LVL_4_ROOT_0_0_0_0 and LVL_4_ROOT_0_0_0_1\n",
      "Merging true sibling clusters: LVL_3_ROOT_0_1_0 and LVL_3_ROOT_0_1_1\n",
      "Merging true sibling clusters: LVL_3_ROOT_1_1_0 and LVL_3_ROOT_1_1_1\n",
      "Merging small cluster with existing merged cluster: LVL_3_ROOT_0_0_1\n",
      "Merging small cluster with existing merged cluster: LVL_2_ROOT_1_0\n",
      "Starting iteration 2\n",
      "WARNING: 1 clusters still below minimum size after merging:\n",
      "  Level 3, size 3: ['Class14', 'Class11', 'Class7']\n",
      "Force merging 1 remaining small clusters...\n",
      "All clusters meet the minimum size requirement.\n",
      "Refined Clusters: [['Class10', 'Class3', 'Class12', 'Class8'], ['Class6', 'Class4', 'Class1', 'Class9', 'Class2', 'Class13', 'Class5'], ['Class14', 'Class11', 'Class7']]\n",
      "----------------\n",
      "Cluster LVL_4_ROOT_0_0_0_0: ['Class10', 'Class12']\n",
      "Cluster LVL_4_ROOT_0_0_0_1: ['Class3', 'Class8']\n",
      "Cluster LVL_3_ROOT_0_0_1: ['Class1', 'Class9']\n",
      "Cluster LVL_2_ROOT_0_1: ['Class2', 'Class4', 'Class5']\n",
      "Cluster LVL_2_ROOT_1_0: ['Class13', 'Class6']\n",
      "Cluster LVL_3_ROOT_1_1_0: ['Class11', 'Class14']\n",
      "Cluster LVL_3_ROOT_1_1_1: ['Class7']\n",
      "Structure changed at observation 1200\n",
      "Labelset Size Distribution: {4: 549, 2: 209, 3: 57, 5: 78, 7: 52, 6: 215, 8: 21, 1: 13, 9: 4, 10: 2}\n",
      "Recommended Minimum Cluster Size: 4\n",
      "Starting iteration 1\n",
      "Found 7 small clusters in iteration 1\n",
      "Merging true sibling clusters: LVL_4_ROOT_0_0_0_0 and LVL_4_ROOT_0_0_0_1\n",
      "Merging true sibling clusters: LVL_3_ROOT_1_1_0 and LVL_3_ROOT_1_1_1\n",
      "Merging small cluster with existing merged cluster: LVL_3_ROOT_0_0_1\n",
      "Merging small cluster with existing merged cluster: LVL_2_ROOT_0_1\n",
      "Merging small cluster with existing merged cluster: LVL_2_ROOT_1_0\n",
      "Starting iteration 2\n",
      "All clusters meet the minimum size requirement.\n",
      "Refined Clusters: [['Class10', 'Class3', 'Class12', 'Class8'], ['Class6', 'Class4', 'Class1', 'Class9', 'Class7', 'Class14', 'Class11', 'Class2', 'Class13', 'Class5']]\n",
      "----------------\n",
      "Cluster LVL_4_ROOT_0_0_0_0: ['Class10', 'Class12']\n",
      "Cluster LVL_4_ROOT_0_0_0_1: ['Class3', 'Class8']\n",
      "Cluster LVL_3_ROOT_0_0_1: ['Class1', 'Class9']\n",
      "Cluster LVL_3_ROOT_0_1_0: ['Class4']\n",
      "Cluster LVL_3_ROOT_0_1_1: ['Class2', 'Class5']\n",
      "Cluster LVL_2_ROOT_1_0: ['Class13', 'Class6']\n",
      "Cluster LVL_2_ROOT_1_1: ['Class11', 'Class14', 'Class7']\n",
      "Structure changed at observation 1300\n",
      "Labelset Size Distribution: {4: 600, 2: 226, 3: 58, 5: 84, 7: 56, 6: 232, 8: 25, 1: 13, 9: 4, 10: 2}\n",
      "Recommended Minimum Cluster Size: 4\n",
      "Starting iteration 1\n",
      "Found 7 small clusters in iteration 1\n",
      "Merging true sibling clusters: LVL_4_ROOT_0_0_0_0 and LVL_4_ROOT_0_0_0_1\n",
      "Merging true sibling clusters: LVL_3_ROOT_0_1_0 and LVL_3_ROOT_0_1_1\n",
      "Merging small cluster with existing merged cluster: LVL_3_ROOT_0_0_1\n",
      "Merging true sibling clusters: LVL_2_ROOT_1_0 and LVL_2_ROOT_1_1\n",
      "Starting iteration 2\n",
      "All clusters meet the minimum size requirement.\n",
      "Refined Clusters: [['Class10', 'Class3', 'Class12', 'Class8'], ['Class2', 'Class4', 'Class1', 'Class9', 'Class5'], ['Class6', 'Class7', 'Class13', 'Class14', 'Class11']]\n",
      "----------------\n",
      "Cluster LVL_4_ROOT_0_0_0_0: ['Class10', 'Class12']\n",
      "Cluster LVL_4_ROOT_0_0_0_1: ['Class3', 'Class8']\n",
      "Cluster LVL_3_ROOT_0_0_1: ['Class1', 'Class9']\n",
      "Cluster LVL_2_ROOT_0_1: ['Class2', 'Class4', 'Class5']\n",
      "Cluster LVL_2_ROOT_1_0: ['Class13', 'Class6']\n",
      "Cluster LVL_3_ROOT_1_1_0: ['Class11']\n",
      "Cluster LVL_3_ROOT_1_1_1: ['Class14', 'Class7']\n",
      "Structure changed at observation 1400\n",
      "Labelset Size Distribution: {4: 643, 2: 239, 3: 63, 5: 95, 7: 57, 6: 252, 8: 29, 1: 15, 9: 4, 10: 3}\n",
      "Recommended Minimum Cluster Size: 4\n",
      "Starting iteration 1\n",
      "Found 7 small clusters in iteration 1\n",
      "Merging true sibling clusters: LVL_4_ROOT_0_0_0_0 and LVL_4_ROOT_0_0_0_1\n",
      "Merging true sibling clusters: LVL_3_ROOT_1_1_0 and LVL_3_ROOT_1_1_1\n",
      "Merging small cluster with existing merged cluster: LVL_3_ROOT_0_0_1\n",
      "Merging small cluster with existing merged cluster: LVL_2_ROOT_0_1\n",
      "Merging small cluster with existing merged cluster: LVL_2_ROOT_1_0\n",
      "Starting iteration 2\n",
      "All clusters meet the minimum size requirement.\n",
      "Refined Clusters: [['Class10', 'Class3', 'Class12', 'Class8'], ['Class6', 'Class4', 'Class1', 'Class9', 'Class7', 'Class14', 'Class11', 'Class2', 'Class13', 'Class5']]\n",
      "----------------\n",
      "Cluster LVL_4_ROOT_0_0_0_0: ['Class10', 'Class12']\n",
      "Cluster LVL_4_ROOT_0_0_0_1: ['Class3', 'Class8']\n",
      "Cluster LVL_3_ROOT_0_0_1: ['Class1', 'Class9']\n",
      "Cluster LVL_3_ROOT_0_1_0: ['Class2', 'Class4']\n",
      "Cluster LVL_3_ROOT_0_1_1: ['Class5']\n",
      "Cluster LVL_2_ROOT_1_0: ['Class13', 'Class6']\n",
      "Cluster LVL_3_ROOT_1_1_0: ['Class11']\n",
      "Cluster LVL_3_ROOT_1_1_1: ['Class14', 'Class7']\n",
      "Structure changed at observation 1500\n",
      "Labelset Size Distribution: {4: 688, 2: 258, 3: 67, 5: 97, 7: 59, 6: 274, 8: 32, 1: 17, 9: 5, 10: 3}\n",
      "Recommended Minimum Cluster Size: 4\n",
      "Starting iteration 1\n",
      "Found 8 small clusters in iteration 1\n",
      "Merging true sibling clusters: LVL_4_ROOT_0_0_0_0 and LVL_4_ROOT_0_0_0_1\n",
      "Merging true sibling clusters: LVL_3_ROOT_0_1_0 and LVL_3_ROOT_0_1_1\n",
      "Merging true sibling clusters: LVL_3_ROOT_1_1_0 and LVL_3_ROOT_1_1_1\n",
      "Merging small cluster with existing merged cluster: LVL_3_ROOT_0_0_1\n",
      "Merging small cluster with existing merged cluster: LVL_2_ROOT_1_0\n",
      "Starting iteration 2\n",
      "WARNING: 1 clusters still below minimum size after merging:\n",
      "  Level 3, size 3: ['Class14', 'Class11', 'Class7']\n",
      "Force merging 1 remaining small clusters...\n",
      "All clusters meet the minimum size requirement.\n",
      "Refined Clusters: [['Class10', 'Class3', 'Class12', 'Class8'], ['Class6', 'Class4', 'Class1', 'Class9', 'Class2', 'Class13', 'Class5'], ['Class14', 'Class11', 'Class7']]\n",
      "----------------\n",
      "Done!\n",
      "Global Metrics: [0.013239553165080699, 0.05437611418113805, 0.14015959548076165, 0.10509617721036844, 0.6783793368402377]\n",
      "Model 0 metric: [0.1427714457108578, 0.36104816652243504, 0.620821833793315, 0.4890875071738897, 0.7880709572371247]\n"
     ]
    }
   ],
   "source": [
    "#DATASTE\n",
    "stream = Yeast() #  Scene() #  Hypersphere() # Yeast() # Emotions() Yeast()  Chd #-----CHANGE HERE\n",
    "\n",
    "#----------///------\n",
    "\n",
    "majority_labelset = MajorityLabelset()\n",
    "\n",
    "color_list = ['#FF5733', '#33FF57', '#FF33A8', '#FF7F50', '#DAF7A6', \n",
    "              '#8A2BE2', '#7FFF00', '#D2691E', '#6495ED', '#FF1493', \n",
    "              '#00FFFF', '#FFD700', '#F08080', '#90EE90', '#FF6347']\n",
    "              \n",
    "#----------///------\n",
    "dataset=stream\n",
    "dataset_name = dataset.filename\n",
    "output_dir = os.path.join(\"tree_plots\", f\"{dataset_name}\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "metrics_csv_path = os.path.join(output_dir, \"final_metrics.csv\")\n",
    "drifts_csv_path = os.path.join(output_dir, \"drifts.csv\")\n",
    "drifts_data = [[\"Observation Index\", \"Num clusters\", \"New Ensemble Nodes\"]]\n",
    "\n",
    "sample_count = dataset.n_samples\n",
    "odac2 = ODAC2()\n",
    "\n",
    "accuracy_over_time = []\n",
    "drift_detection_indexes = []\n",
    "\n",
    "X = 1\n",
    "models = {i: MLHAT() for i in range(1)}\n",
    "\n",
    "global_metric = Metrics([ExactMatch(), MacroAverage(F1()), MicroAverage(F1()), metrics.multioutput.SampleAverage(metrics.Jaccard()), metrics.multioutput.SampleAverage(metrics.Accuracy())])\n",
    "\n",
    "individual_metrics = {i: Metrics([ExactMatch(), MacroAverage(F1()), MicroAverage(F1()), metrics.multioutput.SampleAverage(metrics.Jaccard()), metrics.multioutput.SampleAverage(metrics.Accuracy())]) for i in range(X)}\n",
    "\n",
    "aux = 0\n",
    "t = 50\n",
    "i = 0\n",
    "analyze_at_index = 300\n",
    "\n",
    "\n",
    "ALL_LABELS = list(next(iter(dataset))[1].keys())\n",
    "model_labels = {0: ALL_LABELS}\n",
    "\n",
    "for x, y in dataset:\n",
    "    i += 1\n",
    "    aux += 1\n",
    "    odac2.learn_one(y)\n",
    "\n",
    "    label_sum = defaultdict(float)\n",
    "    label_count = defaultdict(int)\n",
    "\n",
    "    for model_id, model in models.items():\n",
    "        relevant_labels = model_labels.get(model_id, [])\n",
    "        y_filtered = {label: y[label] for label in relevant_labels if label in y}\n",
    "        y_pred = model.predict_proba_one(x)\n",
    "        y_pred_filtered = {label: y_pred[label] for label in relevant_labels if label in y_pred}\n",
    "\n",
    "        if aux > t:\n",
    "            y_pred_binary = {label: max(probs, key=probs.get) for label, probs in y_pred_filtered.items()}\n",
    "            individual_metrics[model_id].update(y_filtered, y_pred_binary)\n",
    "\n",
    "        for label, probs in y_pred_filtered.items():\n",
    "            label_sum[label] += int(probs.get(True, 0.0) == True)\n",
    "            label_count[label] += 1\n",
    "\n",
    "        model.learn_one(x, y_filtered)\n",
    "\n",
    "    y_pred_avg = {label: (label_sum[label] / label_count[label]) if label_count[label] > 0 else 0.0 for label in label_sum}\n",
    "    if y_pred_avg:\n",
    "        best_label = max(y_pred_avg, key=y_pred_avg.get)\n",
    "        y_pred_binary = {label: (label == best_label) for label in y_pred_avg}\n",
    "    else:\n",
    "        y_pred_binary = {}\n",
    "\n",
    "    for label in y.keys():\n",
    "        if label not in y_pred_binary:\n",
    "            y_pred_binary[label] = False  \n",
    "\n",
    "    global_metric.update(y, y_pred_binary)\n",
    "\n",
    "    exact_match_score = global_metric.get()[0]\n",
    "    accuracy_over_time.append(exact_match_score)\n",
    "\n",
    "\n",
    "    # Train the MajorityLabelset classifier\n",
    "    majority_labelset.learn_one(x, y)\n",
    "\n",
    "\n",
    "    if odac2.structure_changed:\n",
    "        if i > 100:\n",
    "        \n",
    "            if sample_count - i < 100:  \n",
    "                continue\n",
    "\n",
    "            aux = 0\n",
    "            print(f\"Structure changed at observation {i}\")\n",
    "            drift_detection_indexes.append(i)\n",
    "\n",
    "            cluster_labels = odac2._get_cluster_labels_string()   \n",
    "            drifts_data.append([i, len(cluster_labels), cluster_labels])\n",
    "\n",
    "            if i > 50:\n",
    "\n",
    "                freq_table = majority_labelset.get_frequency_table()\n",
    "                size_distribution = analyze_labelset_sizes(freq_table)\n",
    "                safe_min_size = get_safe_min_cluster_size(size_distribution)\n",
    "               \n",
    "                safe_max_size=sample_count/2\n",
    "\n",
    "                cluster_dict = odac2.get_cluster_dict()  \n",
    "                sorted_levels = sort_cluster_levels(cluster_dict) \n",
    "\n",
    "\n",
    "                refined_clusters = merge_and_split_clusters(odac2, cluster_dict, safe_min_size, safe_max_size,output_dir, i, color_list)\n",
    "                print(\"Refined Clusters:\", refined_clusters)\n",
    "\n",
    "        print(\"----------------\")\n",
    "\n",
    "print(\"Done!\")\n",
    "print(\"Global Metrics:\", global_metric.get())\n",
    "\n",
    "for model_id, metric in individual_metrics.items():\n",
    "    print(f\"Model {model_id} metric: {metric.get()}\")\n",
    "\n",
    "# Save CSV\n",
    "metrics_data = [[\"Metric Type\", \"Exact Match\", \"Macro F1\", \"Micro F1\", \"Jaccard\", \"Sample Avg Accuracy\", \"Drift Detection Indexes\"]]\n",
    "global_metrics_values = global_metric.get()\n",
    "metrics_data.append([\"Global\"] + global_metrics_values + [drift_detection_indexes])\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlhat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
